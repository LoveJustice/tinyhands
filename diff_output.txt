diff --git a/application/casedispatcher/libraries/entity_model.py b/application/casedispatcher/libraries/entity_model.py
index e989fd39..b561e814 100644
--- a/application/casedispatcher/libraries/entity_model.py
+++ b/application/casedispatcher/libraries/entity_model.py
@@ -1,87 +1,12 @@
-# entity_model.py
-# author: christo strydom
-import os
-import logging
-from datetime import datetime
 import pandas as pd
 from copy import deepcopy
 import gspread_dataframe as gd
 from .case_dispatcher_logging import setup_logger
 import gspread

-
-import pandas as pd
-from copy import deepcopy
-
-# from .case_dispatcher_logging import setup_logger
-
-
 logger = setup_logger("entity_model_logging", "entity_logging")


-class DataFrameValidator:
-    @staticmethod
-    def standardize_columns(df, base_columns=None):
-        """
-        Standardize DataFrame columns by removing duplicates and ensuring consistency
-        """
-        # Get unique column names while preserving order
-        seen = set()
-        unique_cols = []
-        for col in df.columns:
-            if col not in seen:
-                seen.add(col)
-                unique_cols.append(col)
-            else:
-                # If duplicate, append a unique suffix
-                counter = 1
-                while f"{col}_{counter}" in seen:
-                    counter += 1
-                unique_cols.append(f"{col}_{counter}")
-                seen.add(f"{col}_{counter}")
-
-        # Create new DataFrame with unique column names
-        df_unique = df.copy()
-        df_unique.columns = unique_cols
-
-        # If base_columns provided, ensure all required columns exist
-        if base_columns is not None:
-            for col in base_columns:
-                if col not in df_unique.columns:
-                    df_unique[col] = pd.NA
-
-        return df_unique
-
-    @staticmethod
-    def validate_required_columns(df, required_columns):
-        """
-        Validate that DataFrame has all required columns
-        """
-        missing_columns = [col for col in required_columns if col not in df.columns]
-        return len(missing_columns) == 0, missing_columns
-
-    @staticmethod
-    def safe_concat(dfs, common_columns=None):
-        """
-        Safely concatenate DataFrames ensuring column consistency
-        """
-        if not dfs:
-            return pd.DataFrame()
-
-        # If common_columns not provided, use intersection of all DataFrame columns
-        if common_columns is None:
-            common_columns = set.intersection(*[set(df.columns) for df in dfs])
-
-        # Standardize each DataFrame
-        standardized_dfs = []
-        for df in dfs:
-            std_df = DataFrameValidator.standardize_columns(df[common_columns])
-            standardized_dfs.append(std_df)
-
-        # Concatenate standardized DataFrames
-        return pd.concat(standardized_dfs, ignore_index=True)
-
-
 class GetAttr:
     """
     This is a class which allows objects in its subclasses to be indexed.
@@ -106,20 +31,15 @@ class GetAttr:
 """## The EntityGroup"""


-class EntityGroup:
+class EntityGroup(GetAttr):
+    """This is a class for Victims, Suspects, and Police entity groups with
+    corresponding sheets."""
+
     sheets = []

     def __init__(self, uid, new_cases, active_gsheet, closed_gsheet, gsdfs):
+        logger.info(f"Initializing EntityGroup with uid: {uid}")
         try:
-            logger.info(f"Initializing EntityGroup with uid: {uid}")
-            logger.debug(f"New cases columns: {new_cases.columns.tolist()}")
-            logger.debug(
-                f"Active gsheet columns: {gsdfs[active_gsheet].columns.tolist()}"
-            )
-            logger.debug(
-                f"Closed gsheet columns: {gsdfs[closed_gsheet].columns.tolist()}"
-            )
-
             EntityGroup.sheets.append(self)
             self.uid = uid
             self.new = new_cases
@@ -127,10 +47,11 @@ class EntityGroup:
             self.closed = gsdfs[closed_gsheet]
             self.active_name = active_gsheet
             self.closed_name = closed_gsheet
-
             logger.info(f"Successfully initialized EntityGroup with uid: {uid}")
         except Exception as e:
-            logger.error(f"Error initializing EntityGroup: {str(e)}", exc_info=True)
+            logger.error(
+                f"Error while initializing EntityGroup: {str(e)}", exc_info=True
+            )
             raise

     @classmethod
@@ -203,60 +124,76 @@ class EntityGroup:

     @classmethod
     def move_closed(cls, soc_df):
-        validator = DataFrameValidator()
-
+        """Moves closed cases to closed sheet for each Entity Group instance."""
         for sheet in cls.sheets:
-            logger.info(f"Processing sheet: {sheet.active_name}")
-
-            try:
-                # Validate required columns
-                required_columns = [sheet.uid, "case_status", "case_id"]
-                for df_name, df in [
-                    ("sheet.newcopy", sheet.newcopy),
-                    ("soc_df", soc_df),
-                    ("sheet.closed", sheet.closed),
-                ]:
-                    valid, missing = validator.validate_required_columns(
-                        df, required_columns
-                    )
-                    if not valid:
-                        logger.error(f"Missing columns in {df_name}: {missing}")
-                        continue
-
-                # Standardize all DataFrames before operations
-                sheet.newcopy = validator.standardize_columns(sheet.newcopy)
-                sheet.closed = validator.standardize_columns(sheet.closed)
-
-                # Rest of your existing logic, but using safe_concat for concatenation
-                prev_closed = sheet.newcopy[
-                    sheet.newcopy[sheet.uid].isin(
-                        soc_df[soc_df.arrested == 1][sheet.uid]
-                    )
-                ].copy()
-
-                newly_closed = sheet.gsheet[
-                    sheet.gsheet["case_status"].str.contains("Closed", na=False)
-                ]
-
-                # Safely concatenate all closed cases
-                sheet.closed = validator.safe_concat(
-                    [sheet.closed, prev_closed, newly_closed],
-                    common_columns=sheet.closed.columns,
-                )
+            # Handle previously closed cases
+            logger.info(f"Here is the sheet header: {list(sheet.gsheet)}")
+            logger.info(f"Here is the sheet dir: {dir(sheet)}")
+            prev_closed = sheet.newcopy[
+                sheet.newcopy[sheet.uid].isin(soc_df[soc_df.arrested == 1].suspect_id)
+            ].copy()
+            logger.info(f"Here is the prev_closed header: {list(prev_closed)}")
+            logger.info(f"Here is the size of prev_closed: {list(prev_closed.shape)}")
+            prev_closed["case_status"] = (
+                prev_closed["case_status"].fillna("").astype(str)
+            )
+            prev_closed.loc[:, "case_status"] = "Closed: Already in Legal Cases Sheet"

-                # Remove duplicates
-                sheet.closed = sheet.closed.drop_duplicates(subset=sheet.uid)
+            # Handle newly closed cases
+            newly_closed = sheet.gsheet[
+                sheet.gsheet["case_status"].str.contains("Closed", na=False)
+            ]
+            logger.info(f"Here is the newly_closed header: {list(newly_closed)}")
+            # Add timestamps
+            today = pd.Timestamp.today().normalize()
+            newly_closed["date"] = today
+            newly_closed["supervisor_review"] = today
+
+            # Remove duplicates from prev_closed
+            logger.info(f"Here is sheet.uid: {sheet.uid}")
+            prev_closed = prev_closed[
+                ~prev_closed[sheet.uid].isin(sheet.closed[sheet.uid])
+            ]
+            logger.info(f"Here is list(prev_closed): {list(prev_closed)}")
+            # Get unique columns from sheet.closed
+            target_columns = pd.Index(sheet.closed.columns).drop_duplicates()
+            logger.info(f"Here is target_columns: {target_columns}")
+            # Create temporary copies with unique column names
+            sheet_closed_unique = sheet.closed.loc[:, target_columns]
+
+            prev_closed_unique = prev_closed.loc[
+                :, prev_closed.columns.drop_duplicates()
+            ]
+            logger.info(f"Here is prev_closed.columns.drop_duplicates(): {prev_closed.columns.drop_duplicates()}")
+            newly_closed_unique = newly_closed.loc[
+                :, newly_closed.columns.drop_duplicates()
+            ]
+            logger.info(f"Here is newly_closed.columns.drop_duplicates(): {newly_closed.columns.drop_duplicates()}")

-                # Update active sheet
-                sheet.active = sheet.active[
-                    ~sheet.active[sheet.uid].isin(sheet.closed[sheet.uid])
+            # Ensure all DataFrames have the same columns
+            common_columns = list(
+                set(target_columns)
+                & set(prev_closed_unique.columns)
+                & set(newly_closed_unique.columns)
+            )
+            logger.info(f"Here is common_columns: {common_columns}")
+
+            # Filter to common columns before concatenation
+            sheet.closed = pd.concat(
+                [
+                    sheet_closed_unique[common_columns],
+                    prev_closed_unique[common_columns],
+                    newly_closed_unique[common_columns],
                 ]
+            )

-                logger.info(f"Successfully processed sheet: {sheet.active_name}")
+            # Remove duplicates
+            sheet.closed.drop_duplicates(subset=sheet.uid, inplace=True)

-            except Exception as e:
-                logger.error(f"Error processing sheet {sheet.active_name}: {str(e)}")
-                raise
+            # Remove closed cases from active
+            sheet.active = sheet.active[
+                ~sheet.active[sheet.uid].isin(sheet.closed[sheet.uid])
+            ]

     @classmethod
     def move_closed_depr(cls, soc_df):
diff --git a/application/casedispatcher/llm queries b/application/casedispatcher/llm queries
index b136ae33..8e355718 100644
--- a/application/casedispatcher/llm queries
+++ b/application/casedispatcher/llm queries
@@ -1,34 +1,347 @@
-ConnectionUnavailable: Connection has been closed
-Traceback:
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 552, in _run_script
-    exec(code, module.__dict__)
-File "/Users/user/github_repos/tinyhands/application/lji_social_media/streamlit_fb_friends.py", line 35, in <module>
-    graph = Graph(neo4j_url, user=neo4j_usr, password=neo4j_pwd)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/database.py", line 288, in __init__
-    self.service = GraphService(profile, **settings)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/database.py", line 119, in __init__
-    self._connector = Connector(profile, **connector_settings)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 960, in __init__
-    self._add_pools(*self._initial_routers)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 982, in _add_pools
-    pool = ConnectionPool.open(
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 649, in open
-    seeds = [pool.acquire() for _ in range(init_size or cls.default_init_size)]
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 649, in <listcomp>
-    seeds = [pool.acquire() for _ in range(init_size or cls.default_init_size)]
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 813, in acquire
-    cx = self._connect()
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 764, in _connect
-    cx = Connection.open(self.profile, user_agent=self.user_agent,
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/__init__.py", line 174, in open
-    return Bolt.open(profile, user_agent=user_agent,
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/bolt.py", line 361, in open
-    bolt._hello(user_agent or bolt_user_agent())
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/bolt.py", line 867, in _hello
-    self._audit(response)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/bolt.py", line 812, in _audit
-    self.reset(force=True)
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/bolt.py", line 506, in reset
-    self._assert_open()
-File "/Users/user/github_repos/lji_social_media/venv/lib/python3.9/site-packages/py2neo/client/bolt.py", line 449, in _assert_open
-    raise ConnectionUnavailable("Connection has been closed")
+Assistant I have the following script which I want to provide a high level overview of what it does. Can you help me with that?
+
+```python`
+import pandas as pd
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.pipeline import Pipeline, FeatureUnion
+from sklearn.preprocessing import StandardScaler
+from sklearn.model_selection import train_test_split
+from datetime import date
+from sklearn.ensemble import RandomForestClassifier
+from sklearn.model_selection import GridSearchCV
+from time import time
+from .case_dispatcher_logging import setup_logger
+from pathlib import Path
+import pickle
+from datetime import datetime
+from sklearn.preprocessing import FunctionTransformer
+
+
+logger = setup_logger("model_logging", "model")
+
+
+class TypeSelector(BaseEstimator, TransformerMixin):
+    """
+    A transformer for selecting columns based on data type.
+
+    Parameters
+    ----------
+    dtype : str or list-like of str
+        The data type(s) to include.
+
+    Attributes
+    ----------
+    dtype : str or list-like of str
+        The data type(s) to include.
+
+    Methods
+    -------
+    fit :
+        No operation is performed.
+    transform :
+        Selects columns of the input data frame that have the specified data type(s).
+
+    Example
+    -------
+    >>> selector = TypeSelector("float64")
+    >>> X = pd.DataFrame({"col1": [1, 2, 3], "col2": [0.1, 0.2, 0.3]})
+    >>> selector.transform(X)
+       col2
+    0  0.1
+    1  0.2
+    2  0.3
+    """
+
+    def __init__(self, dtype):
+        self.dtype = dtype
+
+    def fit(self, X, y=None):
+        """No operation is performed."""
+        return self
+
+    def transform(self, X):
+        """
+        Selects columns of the input data frame that have the specified data type(s).
+
+        Parameters
+        ----------
+        X : pd.DataFrame
+            The input data frame.
+
+        Returns
+        -------
+        X_selected : pd.DataFrame
+            The selected columns of the input data frame.
+        """
+        assert isinstance(X, pd.DataFrame)
+        return X.select_dtypes(include=[self.dtype])
+
+
+def save_results(best_model, X_validation):
+    """Pickles model and column names and saves them for later use with a timestamp."""
+
+    # Get current date and time
+
+    datetime_now = datetime.now()
+    # Format the date and time in a string (e.g., '2023_12_07_15_30')
+    timestamp = datetime_now.strftime("%Y_%m_%d_%H_%M")
+
+    # Append timestamp to filenames
+    model_filename = f"models/u21_rf_model_{timestamp}.sav"
+    xcols_filename = f"model_compare_data/xcols_{timestamp}.txt"
+
+    # Save the model
+    pickle.dump(best_model, open(model_filename, "wb"))
+
+    # Save the column names
+    xcols = list(X_validation.columns)
+    with open(xcols_filename, "w") as f:
+        for item in xcols:
+            f.write("%s\n" % item)
+
+
+# You can then call this function with your model and validation data
+# save_results(your_model, your_validation_data)
+def build_transformer():
+    """
+    Builds a transformer that applies a no-operation (identity function) to the data,
+    preserving both the data and column order exactly as is.
+    """
+    # No-op transformer for the entire DataFrame
+    no_op_transformer = FunctionTransformer()
+
+    # Wrap the no-op transformer in a pipeline (optional, depending on further needs)
+    transformer = Pipeline([
+        ("no_op", no_op_transformer),
+    ])
+
+    return transformer
+
+def build_transformer_depr():
+    """
+    Builds a transformer for processing data without altering the original types of boolean and integer columns.
+    """
+    transformer = Pipeline([
+        ("features", FeatureUnion(
+            transformer_list=[
+                ("boolean",
+                    Pipeline([
+                        ("selector", TypeSelector("bool")),
+                        # Potentially add steps if needed, but none are needed now
+                    ])
+                ),
+                ("integers",
+                    Pipeline([
+                        ("selector", TypeSelector('int')),
+                        # No scaling or further steps for integers
+                    ])
+                ),
+                # Add a branch for floats if any specific processing is required for float columns
+            ],
+            n_jobs=1,
+        )),
+    ])
+
+    return transformer
+
+
+
+def remove_recent(soc_df, cutoff_days):
+    """
+    Eliminates cases more recent than the cutoff date.
+
+    Parameters:
+        soc_df (pandas.DataFrame): A dataframe containing case information.
+        cutoff_days (int): The number of days from the present to use as the cutoff point.
+
+    Returns:
+        pandas.DataFrame: A dataframe containing only cases that are older than the cutoff date or that resulted in an arrest.
+    """
+    # Get the current date and format it as a string
+    today = date.today()
+    today.strftime("%m/%d/%Y")
+
+    # Calculate the number of days between today and the interview date for each case
+    # df['Days'] = (today - df.loc[:, 'interview_date']) / np.timedelta64(1, 'D')
+    today = pd.Timestamp(today)
+
+    soc_df["interview_date"] = pd.to_datetime(soc_df["interview_date"])
+    soc_df["Days"] = (today - soc_df.loc[:, "interview_date"]).dt.days
+
+    # Create a new dataframe containing only cases that are older than the cutoff date or that resulted in an arrest
+    sub_df = soc_df[(soc_df["Days"] > cutoff_days) | (soc_df["arrested"] == 1)]
+
+    return sub_df
+
+
+def train_test_val_split(sub_df, te_size=0.2, val_size=0.1):
+    """
+    Splits dataset into training, testing, and validation sets.
+
+    Parameters
+    ----------
+    sub_df : DataFrame
+        The input DataFrame to split.
+    te_size : float, optional
+        The size of the test set, as a fraction of the input data. The default is 0.2.
+    val_size : float, optional
+        The size of the validation set, as a fraction of the training set. The default is 0.1.
+
+    Returns
+    -------
+    tuple
+        Four DataFrames, representing the training, validation, and testing sets.
+    """
+    # Save the 'Arrest' column as y
+    sub_df.to_csv(Path("data/modelbuild_sub_df.csv"))
+    y = sub_df.arrested
+
+    # Remove certain columns from the DataFrame, except 'Arrest'
+    X = sub_df.drop(
+        columns=[
+            "arrested",
+            "sf_number",
+            "Days",
+            "interview_date",
+            "suspect_id",
+            "person_id",
+            "case_notes",
+            "social_media",
+            "master_person_id",
+            "operating_country_id",
+            "country",
+            "sf_number_group",
+        ]
+    )
+
+    # Split the data into training and testing sets
+    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=te_size)
+
+    # Calculate the validation set size as a fraction of the training set
+    val_size = val_size / (1 - te_size)
+
+    # Split the training set into training and validation sets
+    X_train, X_validation, y_train, y_validation = train_test_split(
+        X_train, y_train, test_size=val_size
+    )
+
+    # Return the training, validation, and testing sets
+    return X_train, X_validation, y_train, y_validation
+
+
+def get_cls_pipe(clf=RandomForestClassifier()):
+    """
+    Builds a pipeline with a transformer and a classifier algorithm.
+
+    Args:
+        clf (object, optional): The classifier algorithm to use in the pipeline. Defaults to a RandomForestClassifier.
+
+    Returns:
+        object: A pipeline with a transformer and classifier.
+    """
+    # Build the transformer
+    transformer = build_transformer()
+
+    # Create the pipeline with the transformer and classifier
+    cls_pipeline = Pipeline([("transformer", transformer), ("clf", clf)])
+    logger.info(f"Case Dispatcher 6.0: Built cls pipeline")
+
+    # Return the pipeline
+    return cls_pipeline
+
+
+
+def pipe_predict(cls_pipeline, X_train, y_train, X_validation):
+    """Make predictions with classifier pipeline."""
+    cls_pipeline.fit(X_train, y_train)
+    y_rf = cls_pipeline.predict_proba(X_validation)
+    return y_rf
+
+
+def do_gridsearch(cls_pipeline, X_train, y_train):
+    """
+    Conduct grid search cross-validation on a classifier pipeline to find the best model.
+
+    Args:
+        cls_pipeline: A scikit-learn pipeline object containing a classifier.
+        X_train (array-like): Training data.
+        y_train (array-like): Training labels.
+
+    Returns:
+        sklearn.model_selection._search.GridSearchCV: The best model found by grid search.
+    """
+    # Define the parameter grid for the grid search
+    search_space = [
+        {
+            "clf": [RandomForestClassifier()],
+            "clf__bootstrap": [False, True],
+            "clf__n_estimators": [10, 100],
+            #'clf__max_depth': [5, 10, 20, 30, 40, 50, None],
+            "clf__max_depth": [20, 30],
+            #'clf__max_features': [0.5, 0.6, 0.7, 0.8, 1],
+            "clf__max_features": [0.5, 0.6],
+            "clf__class_weight": ["balanced", None],
+        }
+    ]
+    #'clf__class_weight': ["balanced",
+    #                      "balanced_subsample", None]}]
+    logger.info(f"Case Dispatcher 3.0: Start grid search")
+    # Create a grid search object using the classifier pipeline and parameter grid
+    grid_search = GridSearchCV(cls_pipeline, search_space, cv=4, n_jobs=-1, verbose=1)
+    logger.info(f"Case Dispatcher 3.0: Completed grid search")
+    # Print the parameters being searched
+    print("Performing grid search...")
+    print("parameters:")
+    print(search_space)
+
+    # Start the timer and conduct the grid search
+    t0 = time()
+    best_model = grid_search.fit(X_train, y_train)
+    print("done in %0.3fs" % (time() - t0))
+    print()
+
+    # Get the best parameters from the model
+    best_parameters = best_model.best_estimator_.get_params()["clf"]
+
+    # Print the best score and parameters found by the grid search
+    print("Best score: %0.3f" % grid_search.best_score_)
+    print("Best parameters set:")
+    print(best_parameters)
+
+    # Return the best model
+    return best_model
+
+
+def full_gridsearch_pipe(soc_df, cutoff_days=90):
+    sub_df = remove_recent(soc_df, cutoff_days)
+    X_train, X_validation, y_train, y_validation = train_test_val_split(sub_df)
+    X_train.to_csv("data/case_dispatcher_X_train.csv", index=False)
+    y_train.to_csv("data/case_dispatcher_y_train.csv", index=False)
+    X_validation.to_csv("data/case_dispatcher_X_validation.csv", index=False)
+    y_validation.to_csv("data/case_dispatcher_y_validation.csv", index=False)
+    cls_pipeline = get_cls_pipe()
+    best_model = do_gridsearch(cls_pipeline, X_train, y_train)
+    x_cols = list(X_validation.columns)
+    return best_model, x_cols, X_validation
+
+
+def check_grid_search_cv(soc_df, gscv, cutoff_days):
+    """Check to see if Grid Search CV is on, and if it is run Grid Search CV."""
+    if gscv == "On":
+        best_model, x_cols, X_Validation = full_gridsearch_pipe(soc_df, cutoff_days)
+    return best_model, x_cols, X_Validation
+
+
+def make_new_predictions(df, soc_model, x_cols):
+    """Use existing classifier algorithm on new cases without recalculating
+    best fit."""
+    X = df[df.columns.intersection(x_cols)]
+    df["soc"] = soc_model.predict_proba(X)[:, 1]
+    return df
+
+
+def load_model(dir_name, file_prefix):
+    model_filename = f"{dir_name}/{file_prefix}_model.sav"
+    loaded_model = pickle.load(open(model_filename, "rb"))
+    return loaded_model
diff --git a/application/casedispatcher/pages/update_casedispatcher_sheets.py b/application/casedispatcher/pages/update_casedispatcher_sheets.py
index 2335718d..1feaea79 100644
--- a/application/casedispatcher/pages/update_casedispatcher_sheets.py
+++ b/application/casedispatcher/pages/update_casedispatcher_sheets.py
@@ -38,7 +38,10 @@ from libraries.google_lib import (
     get_matching_spreadsheets,
 )
 import dotenv
+from libraries.case_dispatcher_logging import setup_logger
+import gspread

+logger = setup_logger("update_logging", "update_logging")
 dotenv_file = dotenv.find_dotenv()
 dotenv.load_dotenv(dotenv_file)

@@ -452,6 +455,7 @@ def main():
             st.write(f"Add irf_case_notes")
             EntityGroup.add_irf_notes(irf_case_notes)
             st.write(f"Move closed cases")
+            # logger.info(f"Move closed cases: {uid}")
             EntityGroup.move_closed(case_dispatcher_soc_df)
             EntityGroup.move_other_closed(
                 suspects_entity, police_entity, victims_entity
@@ -552,10 +556,10 @@ def main():
                 ~(filtered_active_cases["case_id"] == "")
             ]
             filtered_active_cases = filtered_active_cases.drop_duplicates()
-
-            EntityGroup.update_gsheets(
-                credentials, st.session_state["spreadsheet_name"], filtered_active_cases
-            )
+            logger.info(f"Do EntityGroup.update_gsheets(credentials, st.session_state['spreadsheet_name'], filtered_active_cases here            # )")
+            # EntityGroup.update_gsheets(
+            #     credentials, st.session_state["spreadsheet_name"], filtered_active_cases
+            # )
             # st.dataframe(active_cases)
             st.write(
                 f"Success! {st.session_state['spreadsheet_name']} has been updated."
diff --git a/diff_output.txt b/diff_output.txt
index 2806823a..5bbe6b17 100644
--- a/diff_output.txt
+++ b/diff_output.txt
@@ -1,467 +0,0 @@
-diff --git a/application/casedispatcher/libraries/entity_model.py b/application/casedispatcher/libraries/entity_model.py
-index e8a551b2..527d57c2 100644
---- a/application/casedispatcher/libraries/entity_model.py
-+++ b/application/casedispatcher/libraries/entity_model.py
-@@ -126,37 +126,50 @@ class EntityGroup(GetAttr):
-     def move_closed(cls, soc_df):
-         """Moves closed cases to closed sheet for each Entity Group instance."""
-         for sheet in cls.sheets:
-+            # Handle previously closed cases
-             prev_closed = sheet.newcopy[
-                 sheet.newcopy[sheet.uid].isin(soc_df[soc_df.arrested == 1].suspect_id)
-             ].copy()
-             prev_closed["case_status"] = prev_closed["case_status"].fillna("").astype(str)
--
-             prev_closed.loc[:, "case_status"] = "Closed: Already in Legal Cases Sheet"
-
--            # Update newly_closed to be data where "Case_Status" contains "Closed"
-+            # Handle newly closed cases
-             newly_closed = sheet.gsheet[sheet.gsheet["case_status"].str.contains("Closed", na=False)]
-
--            # Populate the 'date' column with today's date
-+            # Add timestamps
-             today = pd.Timestamp.today().normalize()
-             newly_closed['date'] = today
--
--            today = pd.Timestamp.today().normalize()
-             newly_closed['supervisor_review'] = today
-
-+            # Remove duplicates from prev_closed
-             prev_closed = prev_closed[
-                 ~prev_closed[sheet.uid].isin(sheet.closed[sheet.uid])
-             ]
-
--            original_cols = list(prev_closed.columns)
--            new_cols = [original_cols[i] + str(i) for i in range(len(original_cols))]
-+            # Get unique columns from sheet.closed
-+            target_columns = pd.Index(sheet.closed.columns).drop_duplicates()
-
--            sheet.closed.columns = new_cols
--            prev_closed.columns = new_cols
--            newly_closed.columns = new_cols
-+            # Create temporary copies with unique column names
-+            sheet_closed_unique = sheet.closed.loc[:, target_columns]
-+            prev_closed_unique = prev_closed.loc[:, prev_closed.columns.drop_duplicates()]
-+            newly_closed_unique = newly_closed.loc[:, newly_closed.columns.drop_duplicates()]
-
--            sheet.closed = pd.concat([sheet.closed, prev_closed, newly_closed])
--            sheet.closed.columns = original_cols
-+            # Ensure all DataFrames have the same columns
-+            common_columns = list(set(target_columns) &
-+                                  set(prev_closed_unique.columns) &
-+                                  set(newly_closed_unique.columns))
-+
-+            # Filter to common columns before concatenation
-+            sheet.closed = pd.concat([
-+                sheet_closed_unique[common_columns],
-+                prev_closed_unique[common_columns],
-+                newly_closed_unique[common_columns]
-+            ])
-+
-+            # Remove duplicates
-             sheet.closed.drop_duplicates(subset=sheet.uid, inplace=True)
-+
-+            # Remove closed cases from active
-             sheet.active = sheet.active[
-                 ~sheet.active[sheet.uid].isin(sheet.closed[sheet.uid])
-             ]
-diff --git a/application/lji_social_media/libraries/claude_prompts.py b/application/lji_social_media/libraries/claude_prompts.py
-index 1520fccd..f0edf420 100644
---- a/application/lji_social_media/libraries/claude_prompts.py
-+++ b/application/lji_social_media/libraries/claude_prompts.py
-@@ -13,102 +13,123 @@ ANALYSIS_STR = """' Return your analysis STRICTLY and exclusively in the followi
- EXTRACTION_PROMPTS = {
-     "phone_number_prompt": "Extract phone numbers from the following text and return your findings as a list:"
- }
--
--
- CLAUDE_PROMPTS = {
--    "assure_prompt": (
--        "Examine the following job advertisement. "
--        "Does the job ad convey a tone or use language that subtly assures applicants they are highly likely to get the job, even if it doesn't explicitly say so? "
--        "Look for phrases or a manner of presenting the opportunity that implies it's a 'sure thing' or guarantees success. "
--    ),
--    "bypass_prompt": (
--        "Analyze the following job advertisement for any mentions of offers to bypass normal job requirements. "
--        "This may include using fraudulent documents, skipping interviews, or not needing a CV. "
--    ),
--    "callback_request_prompt": (
--        "Examine the following job advertisement. Does it ask the candidate to send a message and promise to call back? "
--    ),
--    "drop_off_at_secure_location_prompt": (
--        "Analyze the following job advertisement in detail. Are applicants "
--        "asked to drop off their CVs at a real, secure and official location? "
--    ),
--    "false_organization_prompt": (
--        "Examine the following job advertisement. Does it recruit for an organization that has publicly stated they don't advertise job posts on social media? "
--        " Some of these companies include, but are not limited to, [Shoprite, Woolworths, Capitec Bank, Pick n Pay, Spar, Coca-Cola, Transnet, Sasol]"
--    ),
--    "gender_specific_prompt": (
--        " Examine the following job advertisement."
--        " Does it recruit specifically only females or only males for a job that both male and female applicants would typically qualify for? "
--    ),
--    "illegal_activities_prompt": (
--        "Analyze the following job advertisement for any references to work in illegal or morally questionable activities. "
--    ),
--    "immediate_hiring_prompt": (
--        "Examine the following job advertisement. Does it promise immediate hiring? "
--    ),
--    "job_advert_prompt": (
--        "Analyze the following text carefully. "
--        "Can this be considered a job or recruitment advertisement? "
--    ),
--    "language_switch_prompt": (
--        "Examine the following job advertisement. Does it change from English to other languages in the middle of the post? "
--    ),
--    "link_prompt": (
--        "Analyze the following text. Is there one or more links or urls mentioned? When asked tp do so, extract the ONLY the link(s) as evidence. "
--    ),
--    "multiple_provinces_prompt": (
--        "Analyze the following job advertisement. Does it advertise for positions in several provinces, especially without detail? "
--    ),
--    "no_education_skilled_prompt": (
--        "Analyze the following job advertisement. Does it offer a highly skilled job (e.g., engineering, marketing, finance, IT) "
--        "without requiring any education or experience? "
--    ),
--    "no_location_prompt": (
--        "Examine the following job advertisement. Does it fail to mention a specific job location? "
--    ),
--    "overseas_prompt": (
--        "Examine the following job advertisement. Does it offer positions overseas (i.e. outside of South Africa)? "
--    ),
--    "phone_prompt": (
--        "Analyze the following text. Is there one or more phone numbers mentioned? When asked to do so, extract ONLY the numbers(s) as evidence. "
--    ),
--    "quick_money_prompt": (
--        "Analyze the following job advertisement. Does it promise quick or easy money? "
--    ),
--    "recruit_students_prompt": (
--        "Analyze the following job advertisement. Does it specifically recruit people who are under the age of 18? "
--    ),
--    "recruitment_prompt": (
--        "Analyze the following text. Is it unambiguously a recruitment advert? "
--    ),
--    "requires_references": (
--        "Examine the following job advertisement. Does it require the applicant to provide references?"
--    ),
--    "suspicious_email_prompt": (
--        "Analyze the following job advertisement. Please clearly and precisely differentiate between "
--        "email addresses and links.  If an email IS present, does it appear suspicious in the context of the job advertisement? "
--    ),
--    "target_specific_group_prompt": (
--        "Analyze the following job advertisement. Does it target a specific group of people (e.g., women from a particular country or region)? "
--        "Consider vulnerable groups to include these, but not exclusively,[Shona,Ndebele,Basotho, Tswana', Zulu, Mozambicans, Chewa, Yao]"
--    ),
--    "unprofessional_writing_prompt": (
--        "Analyze the following job advertisement for signs of unprofessional writing such as poor grammar or spelling. "
--        "Accept that missing spaces or words not separated by a space or spaces in the text, such as 'andcleaners', 'towork', is NOT a sign of unprofessional writing. "
--    ),
--    "unrealistic_hiring_number_prompt": (
--        "Analyze the following job advertisement. Does it claim to be hiring an unrealistically high number of people? "
--    ),
--    "unusual_hours_prompt": (
--        "Examine the following job advertisement. Does it mention unusual or excessive work hours? "
--    ),
--    "vague_description_prompt": (
--        "Examine the job advertisement and assess whether the description is unusually vague or lacks essential details that would typically be expected for the role. "
--        "Consider if the level of detail provided is insufficient or unclear, particularly for specialized or technical positions where more specific "
--        "responsibilities are normally outlined. "
--    ),
-+    "callback_request_prompt": "Examine the following job advertisement. Does it ask the candidate to send a message and promise to call back?",
-+    "dance_bar_prompt": "Analyze the following job advertisement. Does it offer a job position in a dance bar or gentlemen's club?",
-+    "drop_off_at_secure_location_prompt": "Analyze the following job advertisement in detail. Are applicants asked to drop off their CVs at a real and official location in person-, i.e. a physical location?",
-+    "false_organization_prompt": "Examine the following job advertisement. Does it recruit for an organization that has publicly stated they don't advertise job posts on social media? Some of these companies include, but are not limited to, [Shoprite, Woolworths, Capitec Bank, Pick n Pay, Spar, Coca-Cola, Transnet, Sasol]",
-+    "gender_specific_prompt": "Review the following job advertisement. Does it specifically seek only male or only female applicants for a job that is not inherently gender-specific? Exclude scenarios where specifying gender is reasonable, such as caregiving roles requiring a same-gender caregiver for personal care.",
-+    "language_switch_prompt": "Examine the following job advertisement. Does it change from English to other languages in the middle of the post?",
-+    "massage_or_spa_prompt": "Analyze the following job advertisement. Does it offer a job in a massage parlour, spa or similar institution?",
-+    "multiple_provinces_prompt": "Analyze the following job advertisement. Does it advertise for positions in several provinces, especially without detail?",
-+    "no_education_skilled_prompt": "Analyze the following job advertisement. Does it offer a highly skilled job (e.g., engineering, marketing, finance, IT) without requiring any education or experience?",
-+    "no_location_prompt": "Examine the following job advertisement. Does it fail to mention a specific job location?",
-+    "overseas_position_prompt": "Examine the following job advertisement. Does it offer positions overseas (i.e. outside of South Africa)?",
-+    "recruit_minors_prompt": "Analyze the following job advertisement. Does it specifically recruit people who are under the age of 18?",
-+    "soccer_trial_prompt": "Analyze the following job advertisement. Does it offer soccer or other sports trial opportunities for applicants?",
-+    "suspicious_assurance_prompt": "Analyze the job advertisement for unusual or suspicious assurance that the person will be hired more easily or surely than would be typical.",
-+    "suspicious_email_prompt": "Analyze the following job advertisement. Does it contain an email address that appears suspicious in the context of the job advertisement? Make sure to differentiate between email addresses and links and make your assessment only based on email addresses and not links.",
-+    "target_specific_group_prompt": "Analyze the following job advertisement. Does it target a specific group of people (e.g., women from a particular country or region)? Consider vulnerable groups to include these, but not exclusively, [Shona,Ndebele,Basotho, Tswana', Zulu, Mozambicans, Chewa, Yao]",
-+    "unprofessional_writing_prompt": "Analyze the following job advertisement for signs of unprofessional writing such as poor grammar or spelling. Accept that missing spaces or words not separated by a space or spaces in the text, such as 'andcleaners', 'towork', is NOT a sign of unprofessional writing.",
-+    "unrealistic_hiring_number_prompt": "Analyze the following job advertisement. Does it claim to be hiring an unrealistically high number of people?",
- }
-
-+#
-+#
-+# CLAUDE_PROMPTS = {
-+#     "assure_prompt": (
-+#         "Examine the following job advertisement. "
-+#         "Does the job ad convey a tone or use language that subtly assures applicants they are highly likely to get the job, even if it doesn't explicitly say so? "
-+#         "Look for phrases or a manner of presenting the opportunity that implies it's a 'sure thing' or guarantees success. "
-+#     ),
-+#     "bypass_prompt": (
-+#         "Analyze the following job advertisement for any mentions of offers to bypass normal job requirements. "
-+#         "This may include using fraudulent documents, skipping interviews, or not needing a CV. "
-+#     ),
-+#     "callback_request_prompt": (
-+#         "Examine the following job advertisement. Does it ask the candidate to send a message and promise to call back? "
-+#     ),
-+#     "drop_off_at_secure_location_prompt": (
-+#         "Analyze the following job advertisement in detail. Are applicants "
-+#         "asked to drop off their CVs at a real, secure and official location? "
-+#     ),
-+#     "false_organization_prompt": (
-+#         "Examine the following job advertisement. Does it recruit for an organization that has publicly stated they don't advertise job posts on social media? "
-+#         " Some of these companies include, but are not limited to, [Shoprite, Woolworths, Capitec Bank, Pick n Pay, Spar, Coca-Cola, Transnet, Sasol]"
-+#     ),
-+#     "gender_specific_prompt": (
-+#         " Examine the following job advertisement."
-+#         " Does it recruit specifically only females or only males for a job that both male and female applicants would typically qualify for? "
-+#     ),
-+#     "illegal_activities_prompt": (
-+#         "Analyze the following job advertisement for any references to work in illegal or morally questionable activities. "
-+#     ),
-+#     "immediate_hiring_prompt": (
-+#         "Examine the following job advertisement. Does it promise immediate hiring? "
-+#     ),
-+#     "job_advert_prompt": (
-+#         "Analyze the following text carefully. "
-+#         "Can this be considered a job or recruitment advertisement? "
-+#     ),
-+#     "language_switch_prompt": (
-+#         "Examine the following job advertisement. Does it change from English to other languages in the middle of the post? "
-+#     ),
-+#     "link_prompt": (
-+#         "Analyze the following text. Is there one or more links or urls mentioned? When asked tp do so, extract the ONLY the link(s) as evidence. "
-+#     ),
-+#     "multiple_provinces_prompt": (
-+#         "Analyze the following job advertisement. Does it advertise for positions in several provinces, especially without detail? "
-+#     ),
-+#     "no_education_skilled_prompt": (
-+#         "Analyze the following job advertisement. Does it offer a highly skilled job (e.g., engineering, marketing, finance, IT) "
-+#         "without requiring any education or experience? "
-+#     ),
-+#     "no_location_prompt": (
-+#         "Examine the following job advertisement. Does it fail to mention a specific job location? "
-+#     ),
-+#     "overseas_prompt": (
-+#         "Examine the following job advertisement. Does it offer positions overseas (i.e. outside of South Africa)? "
-+#     ),
-+#     "phone_prompt": (
-+#         "Analyze the following text. Is there one or more phone numbers mentioned? When asked to do so, extract ONLY the numbers(s) as evidence. "
-+#     ),
-+#     "quick_money_prompt": (
-+#         "Analyze the following job advertisement. Does it promise quick or easy money? "
-+#     ),
-+#     "recruit_students_prompt": (
-+#         "Analyze the following job advertisement. Does it specifically recruit people who are under the age of 18? "
-+#     ),
-+#     "recruitment_prompt": (
-+#         "Analyze the following text. Is it unambiguously a recruitment advert? "
-+#     ),
-+#     "requires_references": (
-+#         "Examine the following job advertisement. Does it require the applicant to provide references?"
-+#     ),
-+#     "suspicious_email_prompt": (
-+#         "Analyze the following job advertisement. Please clearly and precisely differentiate between "
-+#         "email addresses and links.  If an email IS present, does it appear suspicious in the context of the job advertisement? "
-+#     ),
-+#     "target_specific_group_prompt": (
-+#         "Analyze the following job advertisement. Does it target a specific group of people (e.g., women from a particular country or region)? "
-+#         "Consider vulnerable groups to include these, but not exclusively,[Shona,Ndebele,Basotho, Tswana', Zulu, Mozambicans, Chewa, Yao]"
-+#     ),
-+#     "unprofessional_writing_prompt": (
-+#         "Analyze the following job advertisement for signs of unprofessional writing such as poor grammar or spelling. "
-+#         "Accept that missing spaces or words not separated by a space or spaces in the text, such as 'andcleaners', 'towork', is NOT a sign of unprofessional writing. "
-+#     ),
-+#     "unrealistic_hiring_number_prompt": (
-+#         "Analyze the following job advertisement. Does it claim to be hiring an unrealistically high number of people? "
-+#     ),
-+#     "unusual_hours_prompt": (
-+#         "Examine the following job advertisement. Does it mention unusual or excessive work hours? "
-+#     ),
-+#     "vague_description_prompt": (
-+#         "Examine the job advertisement and assess whether the description is unusually vague or lacks essential details that would typically be expected for the role. "
-+#         "Consider if the level of detail provided is insufficient or unclear, particularly for specialized or technical positions where more specific "
-+#         "responsibilities are normally outlined. "
-+#     ),
-+# }
-+
- prompts = [
-     "suspicious_email_prompt",
-     "recruit_students_prompt",
-@@ -117,26 +138,47 @@ prompts = [
-     "assure_prompt",
- ]
-
-+# RED_FLAGS = [
-+#     "assure_prompt",
-+#     "bypass_prompt",
-+#     "callback_request_prompt",
-+#     "drop_off_at_secure_location_prompt",
-+#     "false_organization_prompt",
-+#     "gender_specific_prompt",
-+#     "immediate_hiring_prompt",
-+#     "language_switch_prompt",
-+#     "multiple_provinces_prompt",
-+#     "no_education_skilled_prompt",
-+#     "no_location_prompt",
-+#     "overseas_prompt",
-+#     "quick_money_prompt",
-+#     "recruit_students_prompt",
-+#     "requires_references",
-+#     "suspicious_email_prompt",
-+#     "target_specific_group_prompt",
-+#     "unprofessional_writing_prompt",
-+#     "unrealistic_hiring_number_prompt",
-+#     "unusual_hours_prompt",
-+#     "vague_description_prompt",
-+# ]
- RED_FLAGS = [
--    "assure_prompt",
--    "bypass_prompt",
-     "callback_request_prompt",
-+    "dance_bar_prompt",
-     "drop_off_at_secure_location_prompt",
-     "false_organization_prompt",
-     "gender_specific_prompt",
--    "immediate_hiring_prompt",
-     "language_switch_prompt",
-+    "massage_or_spa_prompt",
-     "multiple_provinces_prompt",
-     "no_education_skilled_prompt",
-     "no_location_prompt",
--    "overseas_prompt",
--    "quick_money_prompt",
--    "recruit_students_prompt",
--    "requires_references",
-+    "overseas_position_prompt",
-+    "recruit_minors_prompt",
-+    "sense_of_urgency_prompt",
-+    "soccer_trial_prompt",
-+    "suspicious_assurance_prompt",
-     "suspicious_email_prompt",
-     "target_specific_group_prompt",
-     "unprofessional_writing_prompt",
-     "unrealistic_hiring_number_prompt",
--    "unusual_hours_prompt",
--    "vague_description_prompt",
- ]
-diff --git a/application/lji_social_media/redflags/redflag_maintenance_asyncio.py b/application/lji_social_media/redflags/redflag_maintenance_asyncio.py
-index 63f1296b..61c18d2e 100644
---- a/application/lji_social_media/redflags/redflag_maintenance_asyncio.py
-+++ b/application/lji_social_media/redflags/redflag_maintenance_asyncio.py
-@@ -143,7 +143,7 @@ async def process_advert_async(IDn: int, prompt_name: str) -> None:
-
-         # 3. Do the analysis - this can stay async
-         response = await loop.run_in_executor(
--            None, lf.analyse_advert, chat_engine, advert, prompt_name
-+            None, lf.analyse_advert, chat_engine, prompt_name
-         )
-         if response.result == "error":
-             logger.error(f"Analysis failed for IDn {IDn}: {response.explanation}")
-@@ -174,7 +174,7 @@ async def process_batch_async(adverts: pd.DataFrame, prompt_name: str) -> None:
-
- def query_adverts(prompt_name: str) -> pd.DataFrame:
-     query = """
--    MATCH (g:Group)-[:HAS_POSTING]-(n:Posting)
-+    MATCH (g:Group)-[:HAS_POSTING]-(n:RecruitmentAdvert)
-     WHERE g.country_id = 1
-       AND n.text IS NOT NULL
-       AND n.text <> ""
-@@ -190,7 +190,7 @@ def query_adverts(prompt_name: str) -> pd.DataFrame:
-
-
- async def main_async():
--    prompt_names = cp.RED_FLAGS
-+    # prompt_names = cp.RED_FLAGS
-     prompt_names = cp.CLAUDE_PROMPTS.keys()
-
-     for prompt_name in prompt_names:
-diff --git a/application/lji_social_media/redflags/redflag_ml_deeplearning.py b/application/lji_social_media/redflags/redflag_ml_deeplearning.py
-index a2103609..c2462a6e 100644
---- a/application/lji_social_media/redflags/redflag_ml_deeplearning.py
-+++ b/application/lji_social_media/redflags/redflag_ml_deeplearning.py
-@@ -546,7 +546,7 @@ def main():
-         raise ValueError("Original data must contain 'IDn' column")
-
-     # Load the same splits as ensemble model
--    splits_timestamp = "20241114_115648"
-+    splits_timestamp = "20241204_122617"
-     splits, split_info = load_splits(splits_timestamp)
-
-     # Generate unique model identifier
-diff --git a/application/lji_social_media/redflags/simplified_redflag_model.py b/application/lji_social_media/redflags/simplified_redflag_model.py
-index 156b075d..e7295993 100644
---- a/application/lji_social_media/redflags/simplified_redflag_model.py
-+++ b/application/lji_social_media/redflags/simplified_redflag_model.py
-@@ -455,7 +455,7 @@ def main():
-     Main function with corrected ID handling.
-     """
-     # Load original data with proper ID handling
--    file_path = "../results/advert_flags.csv"
-+    file_path = "results/advert_flags.csv"
-     print("Loading original data...")
-     original_data = pd.read_csv(file_path)
-
-@@ -464,7 +464,7 @@ def main():
-         raise ValueError("Original data must contain 'IDn' column")
-
-     # Load splits
--    splits_timestamp = "20241114_115648"
-+    splits_timestamp = "20241204_122617"
-     splits, split_info = load_splits(splits_timestamp)
-
-     # Generate unique model identifier
-diff --git a/application/lji_social_media/streamlit_fb_friends.py b/application/lji_social_media/streamlit_fb_friends.py
-index 92ac350e..770eeb4d 100644
---- a/application/lji_social_media/streamlit_fb_friends.py
-+++ b/application/lji_social_media/streamlit_fb_friends.py
-@@ -3,7 +3,7 @@ from oauth2client.service_account import ServiceAccountCredentials
- import pandas as pd
- from gspread_dataframe import set_with_dataframe
- import gspread
--
-+import libraries.neo4j_lib as nl
- from selenium.webdriver.common.action_chains import ActionChains
- from selenium.webdriver.remote.webdriver import WebDriver
- from selenium.webdriver.common.by import By
-@@ -18,21 +18,10 @@ import subprocess
- from random import randint
- import re
- from bs4 import BeautifulSoup
--from py2neo import Graph
--from datetime import datetime
--from selenium.common.exceptions import NoSuchElementException, TimeoutException
--
-
--# Environment variable fetching and error handling
--neo4j_url = os.environ.get("NEO4J_URL")
--neo4j_usr = os.environ.get("NEO4J_USR", "neo4j")  # Default to 'neo4j' if not set
--neo4j_pwd = os.environ.get("NEO4J_PWD")
-
--if not all([neo4j_url, neo4j_usr, neo4j_pwd]):
--    raise EnvironmentError("Required NEO4J environment variables are not set.")
--
--# Initialize Graph
--graph = Graph(neo4j_url, user=neo4j_usr, password=neo4j_pwd)
-+from datetime import datetime
-+from selenium.common.exceptions import NoSuchElementException, TimeoutException
-
-
- from social_media.social_media import (
-@@ -446,8 +435,7 @@ CALL apoc.create.addLabels(p, CASE WHEN NOT 'SourceProfile' IN labels(p) THEN ['
- CALL apoc.create.addLabels(f, CASE WHEN $follower_url = $source_url AND NOT 'FollowerProfile' IN labels(f) THEN ['FollowerProfile'] ELSE [] END) YIELD node as fNode
- RETURN pNode, fNode
- """
--
--        graph.run(query, parameters).data()
-+        nl.execute_neo4j_query(query, parameters)
-         profile_dicts.append(parameters)
-     pd.DataFrame(profile_dicts).to_csv(
-         "results/follower_profile_dicts.csv", index=False
-@@ -490,7 +478,7 @@ CALL apoc.create.addLabels(f, CASE WHEN $friend_url = $source_url AND NOT 'Frien
- RETURN pNode, fNode
- """
-
--        graph.run(query, parameters).data()
-+        nl.execute_neo4j_query(query, parameters)
-         profile_dicts.append(parameters)
-     pd.DataFrame(profile_dicts).to_csv("results/profile_dicts.csv", index=False)
-     logging.info("Successfully created results/profile_dicts.csv")
