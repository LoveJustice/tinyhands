{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from libraries.claude_prompts import RED_FLAGS\n",
    "import pickle\n",
    "import json\n",
    "import libraries.neo4j_lib as nl\n",
    "import libraries.claude_prompts as cp\n",
    "import libraries.llm_functions as lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from typing import Any, List, Dict, Optional\n",
    "from openai import RateLimitError  # Correct import\n",
    "import backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_engine(\n",
    "    advert: str, memory: ChatMemoryBuffer, llm_instance: Any\n",
    ") -> Optional[Any]:\n",
    "    documents = [Document(text=advert)]\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    return index.as_chat_engine(\n",
    "        chat_mode=\"context\",\n",
    "        llm=llm_instance,\n",
    "        memory=memory,\n",
    "        system_prompt=(\n",
    "            \"As a career forensic analyst, you have deep insight into crime \"\n",
    "            \"and criminal activity, especially human trafficking. Investigate \"\n",
    "            \"the online recruitment advert and extract pertinent details.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm() -> Any:\n",
    "    # if prompt_name == \"assure_prompt\":\n",
    "    #     return Ollama(\n",
    "    #         model=\"llama3.1:latest\",\n",
    "    #         temperature=0,\n",
    "    #         max_tokens=64768,\n",
    "    #         request_timeout=120.0,\n",
    "    #     )\n",
    "\n",
    "    return OpenAI(temperature=0, model=\"gpt-4o-mini\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDn = 573388\n",
    "prompt_name = 'bypass_prompt'\n",
    "advert, result = nl.get_neo4j_advert_analysis(IDn, prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audit_prompt(prompt_name: str, result: str) -> str:\n",
    "    prompt_name = cp.CLAUDE_PROMPTS[prompt_name]\n",
    "    audit_prompt = f\"Is the correct answer to the following question\\n\\n ```{prompt_name}```\\n\\ {result}\\n\\n?\"\n",
    "    return audit_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "llm_instance = get_llm()\n",
    "chat_engine = create_chat_engine(advert, memory, llm_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_prompt = create_audit_prompt(prompt_name, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lf.audit_analysis(chat_engine, audit_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl.write_audit_to_neo4j(IDn, prompt_name, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (g:Group)-[:HAS_POSTING]-(n:Posting)-[:HAS_ANALYSIS {type: $prompt_name}]-(analysis:Analysis)\n",
    "WHERE g.country_id = 1\n",
    "  AND n.text IS NOT NULL\n",
    "  AND n.text <> \"\"\n",
    "  AND (NOT EXISTS {\n",
    "    MATCH (analysis)-[:HAS_AUDIT {type: $prompt_name}]-(:Audit)\n",
    "  })\n",
    "RETURN ID(n) AS IDn, n.post_id AS post_id, n.text AS advert\n",
    "\"\"\"\n",
    "result = nl.execute_neo4j_query(query, {\"prompt_name\": prompt_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_splits(timestamp):\n",
    "    \"\"\"\n",
    "    Load saved data splits for model training and evaluation.\n",
    "    \"\"\"\n",
    "    splits = {}\n",
    "    for split_type in [\n",
    "        \"X_train\",\n",
    "        \"X_holdout\",\n",
    "        \"y_train\",\n",
    "        \"y_holdout\",\n",
    "        \"X_full\",\n",
    "        \"y_full\",\n",
    "    ]:\n",
    "        with open(f\"data/splits/{split_type}_{timestamp}.pkl\", \"rb\") as f:\n",
    "            splits[split_type] = pickle.load(f)\n",
    "\n",
    "    with open(f\"data/splits/split_info_{timestamp}.json\", \"r\") as f:\n",
    "        split_info = json.load(f)\n",
    "\n",
    "    return splits, split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_distributions(X, feature_names):\n",
    "    \"\"\"Analyze and visualize feature distributions and sparsity\"\"\"\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    stats_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'count': [X[col].sum() for col in feature_names],\n",
    "        'sparsity': [1 - (X[col].sum() / len(X)) for col in feature_names],\n",
    "        'unique_values': [len(X[col].unique()) for col in feature_names]\n",
    "    })\n",
    "    \n",
    "    # Sort by sparsity\n",
    "    stats_df = stats_df.sort_values('sparsity', ascending=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.barplot(data=stats_df, x='sparsity', y='feature')\n",
    "    plt.title('Feature Sparsity Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_sparsity.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create feature presence heatmap\n",
    "    presence_matrix = X[feature_names].astype(bool).astype(int)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(presence_matrix.iloc[:50], # Show first 50 samples\n",
    "                xticklabels=feature_names,\n",
    "                yticklabels=False,\n",
    "                cmap='YlOrRd')\n",
    "    plt.title('Feature Presence Patterns (First 50 Samples)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_patterns.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def analyze_feature_importance(X, y, feature_names):\n",
    "    \"\"\"Analyze feature importance and correlation with target\"\"\"\n",
    "    \n",
    "    correlations = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'correlation': [abs(X[col].corr(y)) for col in feature_names],\n",
    "        'mutual_info': mutual_info_regression(X, y)\n",
    "    }).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=correlations, x='correlation', y='mutual_info')\n",
    "    for i, row in correlations.iterrows():\n",
    "        plt.annotate(row['feature'], (row['correlation'], row['mutual_info']))\n",
    "    plt.title('Feature Importance Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def evaluate_balancing_strategies(X, y, feature_names, model):\n",
    "    \"\"\"Evaluate different balancing strategies\"\"\"\n",
    "    \n",
    "    results = defaultdict(dict)\n",
    "    \n",
    "    # Original baseline\n",
    "    base_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    results['baseline'] = {\n",
    "        'mean_score': base_scores.mean(),\n",
    "        'std_score': base_scores.std()\n",
    "    }\n",
    "    \n",
    "    # Strategy 1: Remove very sparse features\n",
    "    sparse_threshold = 0.95  # Features present in less than 5% of samples\n",
    "    sparse_features = [f for f in feature_names \n",
    "                      if (1 - X[f].sum()/len(X)) > sparse_threshold]\n",
    "    X_no_sparse = X.drop(columns=sparse_features)\n",
    "    scores = cross_val_score(model, X_no_sparse, y, cv=5, scoring='r2')\n",
    "    results['remove_sparse'] = {\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std(),\n",
    "        'removed_features': sparse_features\n",
    "    }\n",
    "    \n",
    "    # Strategy 2: SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_smote, y_smote = smote.fit_resample(X, y)\n",
    "        scores = cross_val_score(model, X_smote, y_smote, cv=5, scoring='r2')\n",
    "        results['smote'] = {\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['smote'] = {'error': str(e)}\n",
    "    \n",
    "    # Strategy 3: ADASYN\n",
    "    try:\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "        scores = cross_val_score(model, X_adasyn, y_adasyn, cv=5, scoring='r2')\n",
    "        results['adasyn'] = {\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['adasyn'] = {'error': str(e)}\n",
    "    \n",
    "    # Strategy 4: Feature grouping (example with semantic grouping)\n",
    "    feature_groups = {\n",
    "        'location_related': ['drop_off_at_secure_location_prompt', 'no_location_prompt', 'multiple_provinces_prompt'],\n",
    "        'communication': ['callback_request_prompt', 'suspicious_email_prompt', 'language_switch_prompt'],\n",
    "        'hiring_process': ['immediate_hiring_prompt', 'requires_references', 'unrealistic_hiring_number_prompt'],\n",
    "        'targeting': ['gender_specific_prompt', 'target_specific_group_prompt', 'recruit_students_prompt'],\n",
    "        'job_details': ['vague_description_prompt', 'unusual_hours_prompt', 'overseas_prompt']\n",
    "    }\n",
    "    \n",
    "    X_grouped = X.copy()\n",
    "    for group_name, features in feature_groups.items():\n",
    "        X_grouped[group_name] = X[features].sum(axis=1)\n",
    "        X_grouped = X_grouped.drop(columns=features)\n",
    "    \n",
    "    scores = cross_val_score(model, X_grouped, y, cv=5, scoring='r2')\n",
    "    results['feature_grouping'] = {\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std(),\n",
    "        'groups': feature_groups\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"Visualize comparison of different strategies\"\"\"\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    strategies = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for strategy, metrics in results.items():\n",
    "        if 'mean_score' in metrics:\n",
    "            strategies.append(strategy)\n",
    "            means.append(metrics['mean_score'])\n",
    "            stds.append(metrics['std_score'])\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(strategies, means)\n",
    "    plt.errorbar(strategies, means, yerr=stds, fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Performance Comparison of Different Strategies')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('strategy_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def main_analysis(X, y, feature_names, model):\n",
    "    \"\"\"Run complete analysis pipeline\"\"\"\n",
    "    \n",
    "    print(\"1. Analyzing Feature Distributions...\")\n",
    "    distribution_stats = analyze_feature_distributions(X, feature_names)\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(distribution_stats)\n",
    "    \n",
    "    print(\"\\n2. Analyzing Feature Importance...\")\n",
    "    importance_analysis = analyze_feature_importance(X, y, feature_names)\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(importance_analysis)\n",
    "    \n",
    "    print(\"\\n3. Evaluating Balancing Strategies...\")\n",
    "    strategy_results = evaluate_balancing_strategies(X, y, feature_names, model)\n",
    "    print(\"\\nStrategy Results:\")\n",
    "    for strategy, results in strategy_results.items():\n",
    "        print(f\"\\n{strategy}:\")\n",
    "        print(results)\n",
    "    \n",
    "    print(\"\\n4. Visualizing Results...\")\n",
    "    visualize_results(strategy_results)\n",
    "    \n",
    "    return {\n",
    "        'distribution_stats': distribution_stats,\n",
    "        'importance_analysis': importance_analysis,\n",
    "        'strategy_results': strategy_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = sorted(glob.glob(\"results/metrics_advanced_redflag_model_*.json\"))[-1]\n",
    "simplified = sorted(glob.glob(\"results/metrics_stacking_model_*.json\"))[-1]\n",
    "deep_learning = sorted(glob.glob(\"results/metrics_deep_learning_model_*.json\"))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp='20241114_115648'\n",
    "splits, split_info = load_data_splits(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.keys()\n",
    "X=splits['X_full']\n",
    "y=splits['y_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    \"Original Model\": original,\n",
    "    \"Simplified Model\": simplified,\n",
    "    \"Deep Learning Model\": deep_learning,\n",
    "}\n",
    "\n",
    "# Load metrics\n",
    "metrics = {}\n",
    "for model_name, file_path in model_files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        metrics[model_name] = data[\"holdout_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see the target distribution\n",
    "print(\"Target value distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "\n",
    "# Look at feature co-occurrence\n",
    "correlation_matrix = X[RED_FLAGS].corr()\n",
    "\n",
    "# Check if certain red flags tend to occur together\n",
    "# This helps understand if synthetic samples would be realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
