Below is a detailed markdown write-up that explains the data collection and processing pipeline used to build the arrest model.

---

# Arrest Model Data Collection & Processing Pipeline

This document outlines the steps taken by the data collection function to build an arrest model. The function gathers data from SearchLight and a PostgreSQL database, processes and transforms it, and finally saves the processed datasets to the cloud (Google Drive) for later use in model development.

---

## Overview

The main function of the pipeline:

- **Collects data** from various sources within SearchLight.
- **Processes and cleans** the data to handle missing values, generate dummy variables, and merge related datasets.
- **Transforms** the data to ensure it is in the correct format and type for modeling.
- **Saves the processed data** to the cloud using the Google Drive API.

---

## Data Sources

Data is retrieved from SearchLight via PostgreSQL. The key datasets include:

- **VDF Data:** Victim or case-specific data.
- **Suspects Data:** Information on suspects related to the cases.
- **IRF Data:** Incident Report Form data that contains details such as case notes, dates of interception, and operating country information.
- **Suspect Evaluations:** Evaluations or assessments made on the suspects.

---

## Authentication & Cloud Integration

- **SearchLight Credentials:**  
  The configuration and access tokens are stored in Streamlit’s secrets (`st.secrets`) and are used to authenticate and retrieve data from SearchLight.

- **Google Drive API:**  
  The function converts credentials from the secrets into a format recognized by the Google API. Once processing is complete, datasets are converted into binary (pickled) files and uploaded to Google Drive.

---

## Workflow Breakdown

### 1. User Input & Audit Option

- **User Interface:**  
  A Streamlit sidebar is provided for user interaction. Users can select whether to include an audit for a specific IRF case number.
  
- **Audit Input:**  
  If the audit option is selected, the user can input a case number. This input can be used for logging or additional validation in the processing steps.

---

### 2. Data Extraction

Data is extracted using dedicated functions:
- **VDF Data:** Retrieved using `get_vdf()`.
- **Suspects Data:** Retrieved using `get_suspects()`.
- **IRF Data:** Retrieved using `get_irf()`.
- **Suspect Evaluations:** Retrieved using `get_suspect_evaluations()`.

The extraction functions interface with the PostgreSQL database underlying SearchLight.

---

### 3. Data Preprocessing & Transformation

#### a. Suspects Data Processing
- **Missing Values:**  
  The `suspect_arrested` column is filled with `"No"` where missing.
  
- **Role Encoding:**  
  Dummy variables for the suspect’s role are created using `data_prep.create_role_dummies()`.  
  The original `role` column is dropped after concatenating the new dummy columns.
  
- **Arrest Flag:**  
  A new `arrested` flag is generated by checking if the `suspect_arrested` string starts with `"Yes"`.  
  Columns such as `suspect_arrested` and `arrest_date` are then removed.

#### b. Suspect Evaluations Processing
- **Removing Duplicates:**  
  Duplicate rows in the suspect evaluations dataset are removed.
  
- **Pivoting Evaluations:**  
  Evaluations are pivoted into a wide format where each evaluation type becomes a boolean column.  
  Column names are standardized (e.g., renaming “Definitely trafficked many people” to `pv_believes_definitely_trafficked_many`).

#### c. IRF Data Processing
- **Case Notes & Date Conversion:**  
  Only the relevant columns (`irf_number`, `case_notes`, `operating_country_id`, `date_of_interception`) are extracted.  
  Dates are converted to datetime objects, and the number of days since interception is calculated using today’s date.

#### d. Data Merging
- **Merging Suspects and IRF Data:**  
  A `case_id` is extracted from the suspect’s `sf_number` (using `extract_case_id()`), and used to merge IRF data with suspects data.  
  The pivoted suspect evaluations are also merged into the suspects data.
  
- **Mapping VDF Data:**  
  Certain columns in VDF data (e.g., `pv_recruited_how`, `pv_expenses_paid_how`) are processed using mappings to generate additional columns, after which the original columns are dropped.
  
- **Victim Data:**  
  The VDF data is filtered (e.g., via a query on `role`) to create victim-specific data, which is then merged into the suspects dataset.

#### e. Additional Feature Engineering
- **Gender Encoding:**  
  The `gender` column is transformed into dummy variables, and the columns are renamed (e.g., `female`, `male`, `unknown_gender`).
  
- **Handling Numerical Features:**  
  Missing values in the `age` column are filled with a placeholder value (e.g., -99).  
  Other numerical features (like `job_promised_amount` and `days`) are also processed.
  
- **Country Statistics:**  
  External country-level statistics are added by merging with data from a CSV file using `data_prep.add_country_stats()`.

---

### 4. Final Data Preparation

- **Data Type Conversion:**  
  Boolean features are cast to boolean types, and numerical features are ensured to be integers.
  
- **Cleaning:**  
  Columns that contain only `False` values are removed, and duplicate rows are dropped from the final dataset.
  
- **Final Dataset:**  
  The resulting consolidated dataset (`soc_df`) is designed for model building. Additional datasets for victims, suspects, and IRF case notes are also maintained for further analysis.

---

### 5. Saving Processed Data to Cloud

- **Conversion & Saving:**  
  The processed dataframes are converted to byte streams using `make_file_bytes()`.
  
- **Uploading:**  
  The byte streams are uploaded to Google Drive via the `save_to_cloud()` function, creating files such as:
  - `case_dispatcher_soc_df.pkl`
  - `new_victims.pkl`
  - `new_suspects.pkl`
  - `irf_case_notes.pkl`
  
- **Verification:**  
  After uploading, the function outputs the file IDs for verification.

---

## Summary

This pipeline is an end-to-end process that:

- **Extracts** raw data from SearchLight’s PostgreSQL database.
- **Transforms** and **merges** data from multiple sources (suspects, IRF, VDF, evaluations) into a unified format.
- **Processes** features to handle missing values, encode categorical data, and compute derived metrics (such as days since interception).
- **Saves** the resulting datasets to Google Drive for subsequent modeling tasks.

By systematically cleaning, merging, and enriching the data, this function ensures that the arrest model is built on high-quality, comprehensive data. This setup ultimately supports more accurate predictive modeling and robust analysis.

---